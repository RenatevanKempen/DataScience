{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via the site 'Beingdatum.com'  I've followed the course: 'Guide on Deep Learning for NLP'. \n",
    "\n",
    "This Notebook is a summary of that course, which I will use as reference work when having questions in a NLP project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\renate\\anaconda3\\anaconda\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied: six in c:\\users\\renate\\anaconda3\\anaconda\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\users\\renate\\anaconda3\\anaconda\\lib\\site-packages (from nltk) (3.4.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization\n",
    "The process of segmenting running text into words and sentences\n",
    "\n",
    "### Manually tokenize a textfile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "filename = 'filename.txt' #this is a textfile in which we have written 'Hello World'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "# convert to lowercase\n",
    "words = [word.lower() for word in words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize a textfile with NLTK (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World']\n"
     ]
    }
   ],
   "source": [
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Bag of Words model\n",
    "\n",
    "To create the bag of words model, we need to create a matrix where the columns correspond to the most frequent words in our dictionary where rows correspond to the document or sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Tokenize a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Deep learning methods are popular for natural language, primarily because they are delivering on their promise. Some of the first large demonstrations of the power of deep learning were in natural language processing, specifically speech recognition. More recently in machine translation.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first preprocess the data, in order to:\n",
    "\n",
    "- Convert text to lower case.\n",
    "- Remove all non-word characters.\n",
    "- Remove all punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all needed librairies:\n",
    "import nltk \n",
    "import re \n",
    "import numpy as np \n",
    "dataset = nltk.sent_tokenize(text) \n",
    "for i in range(len(dataset)):     \n",
    "    dataset[i] = dataset[i].lower() #all text in lowercase\n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i]) #Search  any non-alphanumeric character\n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])  #search for all punctuations and split the text into three sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deep learning methods are popular for natural language primarily because they are delivering on their promise ',\n",
       " 'some of the first large demonstrations of the power of deep learning were in natural language processing specifically speech recognition ',\n",
       " 'more recently in machine translation ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Obtaining most frequent words in our text: \n",
    "\n",
    "\n",
    "We will apply the following steps to generate our model:\n",
    "- We declare a dictionary to hold our bag of words. \n",
    "- Next we tokenize each sentence to words. \n",
    "- Now for each word in a sentence, we check if the word exists in our dictionary.\n",
    "- If it does, then we increment its count by 1. If it doesn’t, we add it to our dictionary and set its count as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deep</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>popular</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>natural</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>primarily</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>because</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delivering</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>their</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promise</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>some</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>large</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>demonstrations</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>power</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>were</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>processing</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>specifically</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recognition</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>more</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recently</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>translation</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "deep            2\n",
       "learning        2\n",
       "methods         1\n",
       "are             2\n",
       "popular         1\n",
       "for             1\n",
       "natural         2\n",
       "language        2\n",
       "primarily       1\n",
       "because         1\n",
       "they            1\n",
       "delivering      1\n",
       "on              1\n",
       "their           1\n",
       "promise         1\n",
       "some            1\n",
       "of              3\n",
       "the             2\n",
       "first           1\n",
       "large           1\n",
       "demonstrations  1\n",
       "power           1\n",
       "were            1\n",
       "in              2\n",
       "processing      1\n",
       "specifically    1\n",
       "speech          1\n",
       "recognition     1\n",
       "more            1\n",
       "recently        1\n",
       "machine         1\n",
       "translation     1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2count = {} \n",
    "for data in dataset: \n",
    "    words = nltk.word_tokenize(data) \n",
    "    for word in words: \n",
    "        if word not in word2count.keys(): \n",
    "            word2count[word] = 1\n",
    "        else: \n",
    "            word2count[word] += 1\n",
    "#show the output\n",
    "import pandas as pd\n",
    "table = pd.DataFrame(word2count, index=[0])\n",
    "table.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model, we have a total of 41 words. However when processing large texts, the number of words could reach millions. We do not need to use all those words. Hence, we select a particular number of most frequently used words. To implement this we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'are',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'the',\n",
       " 'in',\n",
       " 'methods',\n",
       " 'popular',\n",
       " 'for',\n",
       " 'primarily',\n",
       " 'because',\n",
       " 'they',\n",
       " 'delivering',\n",
       " 'on',\n",
       " 'their',\n",
       " 'promise',\n",
       " 'some',\n",
       " 'first']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import heapq\n",
    "freq_words = heapq.nlargest(20, word2count, key=word2count.get) #20 denotes the number of words we want. In larger datasets we can set this to larger numbers\n",
    "freq_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3  Building the Bag of Words model:\n",
    "In this step we construct a vector, which would tell us whether a word in each sentence is a frequent word or not. If a word in a sentence is a frequent word, we set it as 1, else we set it as 0.+\n",
    "\n",
    "This can be implemented with the help of following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "for data in dataset:\n",
    "    vector = []\n",
    "    for word in freq_words:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    X.append(vector)\n",
    "X = np.asarray(X)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. N-Grams\n",
    "\n",
    "N-grams of texts are extensively used in text mining and natural language processing tasks. They are basically a set of co-occurring words within a given window and when computing the n-grams you typically move one word forward (although you can move X words forward in more advanced scenarios).\n",
    "- When N=1, this is referred to as unigrams and this is essentially the individual words in a sentence. \n",
    "- When N=2, this is called bigrams \n",
    "- When N=3 this is called trigrams. \n",
    "- When N>3 this is usually referred to as four grams or five grams and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many N-grams in a sentence?\n",
    "If X = Number of words in a given sentence K, the number of N-grams for sentence K would be: \n",
    "\n",
    "${Ngrams_K}= X - (N-1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python code for N-grams\n",
    "def generate_ngrams(text,n): \n",
    "    # split sentences into tokens\n",
    "    tokens=re.split(\"\\\\s+\",text)\n",
    "    ngrams=[] \n",
    "    # collect the n-grams\n",
    "    for i in range(len(tokens)-n+1):\n",
    "     temp=[tokens[j] for j in range(i,i+n)]\n",
    "     ngrams.append(\" \".join(temp)) \n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is', 'is sparta']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ngrams('This is sparta', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding is a dense representation of words in the form of numeric vectors.\n",
    "\n",
    "The word embedding representation is able to reveal many hidden relationships between words. For example, vector(“cat”) – vector(“kitten”) is similar to vector(“dog”) – vector(“puppy”).\n",
    "\n",
    "### Why do we use word embedding?\n",
    "Words aren’t things that computers naturally understand. By encoding them in a numeric form, we can apply mathematical rules and do matrix operations to them. This makes them amazing in the world of machine learning, especially.\n",
    "\n",
    "Take deep learning for example. By encoding words in a numerical form, we can take many deep learning architectures and apply them to words. Convolutional neural networks have been applied to NLP tasks using word embedding and have set the state-of-the-art performance for many tasks.\n",
    "\n",
    "Even better, what we have found is that we can actually pre-train word embedding that are applicable to many tasks. \n",
    "\n",
    "### examples of word embedding:\n",
    "-  <span style=\"text-decoration: underline\">**One-Hot-Encoding (Count Vectorizing)**</span>:\n",
    "\n",
    "Create a vector that has as many dimensions as your corpora has unique words. Each unique word has a unique dimension and will be represented by a 1 in that dimension with 0s everywhere else.\n",
    "- <span style=\"text-decoration: underline\">**TF-IDF Transform**</span>:\n",
    "\n",
    "TF-IDF vectors are related to one-hot encoded vectors. However, instead of just featuring a count, they feature numerical representations where words aren’t just there or not there. Instead, words are represented by their term frequency multiplied by their inverse document frequency.\n",
    "In simpler terms, words that occur a lot but everywhere should be given very little weighting or significance. We can think of this as words like the or and in the English language. They don’t provide a large amount of value.\n",
    "However, if a word appears very little or appears frequently, but only in one or two places, then these are probably more important words and should be weighted as such.\n",
    "Again, this suffers from the downside of very high dimensional representations that don’t capture semantic relatedness.\n",
    "- <span style=\"text-decoration: underline\">**Co-Occurrence Matrix**</span>:\n",
    "\n",
    "A co-occurrence matrix is exactly what it sounds like: a giant matrix that is as long and as wide as the vocabulary size. If words occur together, they are marked with a positive entry. Otherwise, they have a 0. \n",
    "It boils down to a numeric representation that simple asks the question of “Do words occur together? If yes, then count this.”\n",
    "And what can we already see becoming a big problem? Super large representation! If we thought that one-hot encoding was high dimensional, then co-occurrence is high dimensional squared. That’s a lot of data to store in memory\n",
    "\n",
    "Advantages of Co-occurrence Matrix\n",
    "\n",
    "- It preserves the semantic relationship between words. i.e men and women tend to be closer than man and apple.\n",
    "- It uses SVD at its core, which produces more accurate word vector representations than existing methods.\n",
    "- It uses factorization which is a well-defined problem and can be efficiently solved.\n",
    "- It has to be computed once and can be used anytime once computed. In this sense, it is faster in comparison to others.\n",
    "\n",
    "Disadvantages of Co-Occurrence Matrix+\n",
    "\n",
    "- It requires huge memory to store the co-occurrence matrix.\n",
    "- But, this problem can be circumvented by factoring the matrix out of the system for example in Hadoop clusters etc. and can be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example using count vertorizing / one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# To create a Count Vectorizer, we simply need to instantiate one.\n",
    "\n",
    "sample_text = [\"Machine Learning: Introduction to Machine learning and hands-on experience on the various applications of ML\",\n",
    "\"Deep Learning: Introduction to Deep learning & NLP\"]\n",
    "#these are two booktitles\n",
    "\n",
    "#CountVectorizer Plain and Simple\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(sample_text)\n",
    "count_vector=cv.fit_transform(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens above is that the 2 books titles are preprocessed, tokenized and represented as a sparse matrix as explained in the introduction. By default, CountVectorizer does the following:\n",
    "\n",
    "- lowercases your text (set lowercase=false if you don’t want lowercasing)\n",
    "- uses utf-8 encoding\n",
    "- performs tokenization (converts raw text to smaller units of text)\n",
    "- uses word level tokenization (meaning each word is treated as a separate token)\n",
    "- ignores single characters during tokenization (so say bye bye to words like ‘a’ and ‘I’)\n",
    "Now, let’s look at the vocabulary (collection of unique words from our documents):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'machine': 7,\n",
       " 'learning': 6,\n",
       " 'introduction': 5,\n",
       " 'to': 13,\n",
       " 'and': 0,\n",
       " 'hands': 4,\n",
       " 'on': 11,\n",
       " 'experience': 3,\n",
       " 'the': 12,\n",
       " 'various': 14,\n",
       " 'applications': 1,\n",
       " 'of': 10,\n",
       " 'ml': 8,\n",
       " 'deep': 2,\n",
       " 'nlp': 9}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are using all the defaults, these are all word level tokens, lower-cased. Note that the numbers here are not counts, they are the position in the sparse vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 15)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's check the shape:\n",
    "count_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have two rows (two titles) and 15 unique words! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer and Stop Words\n",
    "\n",
    "Now, the first thing you may want to do, is to eliminate stop words from your text as it has limited predictive power and may not help with downstream tasks such as text classification. Stop word removal is a breeze with CountVectorizer and it can be done in several ways:\n",
    "\n",
    "- Use a custom stop word list that you provide\n",
    "- Use sklearn’s built in English stop word list (not recommended)\n",
    "- Create corpora specific stop words using max_df and min_df (highly recommended)\n",
    "\n",
    "Now let’s look at these 3 ways of using stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 11)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Custom stop word list:\n",
    "cv = CountVectorizer(sample_text,stop_words=[\"all\",\"on\",\"the\",\"is\",\"and\",\"to\"])\n",
    "count_vector=cv.fit_transform(sample_text)\n",
    "count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all', 'on', 'the', 'is', 'and', 'to']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the shape has changed from 15 unique words to 11 unique words because the stop words have been removed. \n",
    "#let's see what python has remembered as the stop word list:\n",
    "cv.stop_words\n",
    "\n",
    "# Note that we can actually load stop words directly from a file into a list and supply that as the stop word list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words using MIN_DF:\n",
    "\n",
    "The goal of MIN_DF is to ignore words that have very few occurrences to be considered meaningful. For example, in your text you may have names of people that may appear in only 1 or two documents. In some applications, this may qualify as noise and could be eliminated from further analysis.+\n",
    "\n",
    "Instead of using a minimum term frequency (total occurrences of a word) to eliminate words, MIN_DF looks at _**how many documents contained a term**_, better known as _**document frequency**_. The MIN_DF value can be an _**absolute value**_ (e.g. 1, 2, 3, 4) or a _**value representing proportion of documents**_ (e.g. 0.25 meaning, ignore words that have appeared in 25% of the documents) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and',\n",
       " 'applications',\n",
       " 'deep',\n",
       " 'experience',\n",
       " 'hands',\n",
       " 'machine',\n",
       " 'ml',\n",
       " 'nlp',\n",
       " 'of',\n",
       " 'on',\n",
       " 'the',\n",
       " 'various'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Eliminating words that appeared in less than 2 documents:\n",
    "\n",
    "cv = CountVectorizer(sample_text,min_df=2)\n",
    "count_vector=cv.fit_transform(sample_text)\n",
    "cv.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning': 1, 'introduction': 0, 'to': 2}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To see what’s remaining, all we need to do is check the vocabulary again\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words using MAX_DF:\n",
    "\n",
    "Just as we ignored words that were too rare with MIN_DF, we can ignore words that are too common with MAX_DF. \n",
    "\n",
    "MAX_DF looks at _**how many documents contained a term**_, and if it exceeds the MAX_DF threshold, then it is eliminated from consideration. The MAX_DF value can be an _**absolute value**_ (e.g. 1, 2, 3, 4) or a _**value representing proportion of documents**_ (e.g. 0.85 meaning, ignore words appeared in 85% of the documents as they are too common)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'introduction', 'learning', 'to'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(sample_text,max_df=0.50)\n",
    "count_vector=cv.fit_transform(sample_text)\n",
    "cv.stop_words_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to see which words have been eliminated, you can use cv.stop_words_ (see output above):+\n",
    "\n",
    "In this example, all the words that appeared in all 2 book titles have been eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction based methods:\n",
    "\n",
    "#### 1. Continuous Bag of Words(CBOW) model\n",
    "CBOW is learning to predict the word by the context. A context may be single word or multiple words for a given target words.\n",
    "\n",
    "- Example text “The man jumped over the wall.”\n",
    "\n",
    "CBOW's approach is to treat {“The”, “man”, ’over”, “the’, “wall”} as a context and from these words, be able to predict or generate the center word “jumped”. \n",
    "\n",
    "#### 2. Skip-gram model\n",
    "\n",
    "- Example text “The man jumped over the wall.”\n",
    "\n",
    "Skip-gram's approach is to create a model such that given the center word “jumped”, the model will be able to predict or generate the surrounding words “The”, “man”, “over”, “the”, “wall”. Here we call the word “jumped” the context. \n",
    "\n",
    "Advantages/Disadvantages of CBOW and Skip-gram:\n",
    "\n",
    "- Being probabilistic is nature, these are supposed to perform superior to deterministic methods(generally).\n",
    "- These are low on memory. They don’t need to have huge RAM requirements like that of co-occurrence matrix where it needs to store three huge matrices.\n",
    "- Though CBOW (predict target from context) and skip-gram (predict context words from target) are just inverted methods to each other, they each have their advantages/disadvantages. Since CBOW can use many context words to predict the 1 target word, it can essentially _**smooth out**_ over the distribution. This is essentially like regularization and offer very good performance when our input data is not so large. However the skip-gram model is more _**fine grained**_ so we are able to extract more information and essentially have more accurate embeddings when we have a large data set (large data is always the best regularizer). Skip-gram with negative sub-sampling outperforms every other method generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec using Gensim library\n",
    "\n",
    "Let’s create a corpus using a single wikipedia article. To do so, we need to scrape wikipedia using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\renate\\anaconda3\\anaconda\\lib\\site-packages (4.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml\n",
    "#Used to parse XML and HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The article we are going to scrape is the Wikipedia article on Artificial Intelligence. \n",
    "#Let’s write a Python Script to scrape the article from Wikipedia\n",
    "\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "scrapped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "article = scrapped_data .read()\n",
    "\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs:\n",
    "    article_text += p.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above, we first download the Wikipedia article using the urlopen method of the request class of the urllib library. We then read the article content and parse it using an object of the BeautifulSoup class. Wikipedia stores the text content of the article inside p tags. We use the find_all function of the BeautifulSoup object to fetch all the contents from the paragraph tags of the article.\n",
    "\n",
    "Finally, we join all the paragraphs together and store the scraped article in article_text variable for later use.\n",
    "\n",
    "**Preprocessing**\n",
    "\n",
    "The next step is to preprocess the content for Word2Vec model. The following script preprocess the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text\n",
    "processed_article = article_text.lower()\n",
    "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "\n",
    "# Preparing the dataset\n",
    "all_sentences = nltk.sent_tokenize(processed_article)\n",
    "\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
